# Model Lip Sync siÃªu tháº­t video 4k Ä‘á»™ dÃ i khÃ´ng giá»›i háº¡n

<table>
<tr>
<td width="50%">

![](assets/thl.PNG)

</td>
<td width="50%">

https://github.com/user-attachments/assets/your-video-id-here

*Upload `output/video_out.mp4` as an attachment to get the GitHub asset URL*

</td>
</tr>
</table>

<div align="center">

[![arXiv](https://img.shields.io/badge/arXiv-Paper-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2412.09262)
[![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Model-yellow)](https://huggingface.co/ByteDance/LatentSync-1.6)
[![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Space-yellow)](https://huggingface.co/spaces/fffiloni/LatentSync)
<a href="https://replicate.com/lucataco/latentsync"><img src="https://replicate.com/lucataco/latentsync/badge" alt="Replicate"></a>

*Audio-conditioned lip-sync cháº¥t lÆ°á»£ng cao*

</div>

---

## ğŸ“– Giá»›i thiá»‡u

**LatentSync** lÃ  má»™t phÆ°Æ¡ng phÃ¡p lip-sync end-to-end dá»±a trÃªn audio-conditioned latent diffusion models, khÃ´ng sá»­ dá»¥ng báº¥t ká»³ biá»ƒu diá»…n chuyá»ƒn Ä‘á»™ng trung gian nÃ o. Framework nÃ y táº­n dá»¥ng kháº£ nÄƒng máº¡nh máº½ cá»§a Stable Diffusion Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a trá»±c tiáº¿p má»‘i tÆ°Æ¡ng quan phá»©c táº¡p giá»¯a Ã¢m thanh vÃ  hÃ¬nh áº£nh.

### âœ¨ TÃ­nh nÄƒng ná»•i báº­t

- ğŸ­ **Cháº¥t lÆ°á»£ng lip-sync cao**: Äá»“ng bá»™ mÃ´i tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c
- ğŸš€ **Dá»… dÃ ng sá»­ dá»¥ng**: Há»— trá»£ cáº£ Gradio app vÃ  command line
- ğŸ”§ **Linh hoáº¡t**: Äiá»u chá»‰nh cÃ¡c tham sá»‘ Ä‘á»ƒ tá»‘i Æ°u káº¿t quáº£
- ğŸ’¯ **Hiá»‡u suáº¥t tá»‘t**: Há»— trá»£ nhiá»u Ä‘á»™ phÃ¢n giáº£i vÃ  tá»‘i Æ°u VRAM

### ğŸ–¥ï¸ YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.10
- **CUDA**: 12.4 (khuyáº¿n nghá»‹ cho GPU acceleration)
- **GPU VRAM**: 
  - LatentSync 1.5: Tá»‘i thiá»ƒu 8GB
  - LatentSync 1.6: Tá»‘i thiá»ƒu 18GB

## ğŸ”§ CÃ i Ä‘áº·t

### 1. Táº¡o mÃ´i trÆ°á»ng áº£o
```bash
# Linux/Mac
python3.10 -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
.\venv\Scripts\Activate.ps1
```

### 2. CÃ i Ä‘áº·t dependencies vÃ  táº£i checkpoints
```bash
source setup_env.sh
```

### 3. Kiá»ƒm tra cáº¥u trÃºc checkpoints
Sau khi cÃ i Ä‘áº·t thÃ nh cÃ´ng, checkpoints sáº½ cÃ³ cáº¥u trÃºc nhÆ° sau:

```
./checkpoints/
|-- latentsync_unet.pt
|-- whisper
|   `-- tiny.pt
```

**LÆ°u Ã½:** Báº¡n cÅ©ng cÃ³ thá»ƒ táº£i `latentsync_unet.pt` vÃ  `tiny.pt` thá»§ cÃ´ng tá»« [HuggingFace repo](https://huggingface.co/ByteDance/LatentSync-1.6)

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### PhÆ°Æ¡ng phÃ¡p 1: Gradio App (Giao diá»‡n Ä‘á»“ há»a)

Cháº¡y á»©ng dá»¥ng Gradio Ä‘á»ƒ sá»­ dá»¥ng giao diá»‡n Ä‘á»“ há»a:

```bash
python gradio_app.py
```

### PhÆ°Æ¡ng phÃ¡p 2: Command Line Interface

#### Sá»­ dá»¥ng script cÃ³ sáºµn:
```bash
./inference.sh
```

#### Cháº¡y trá»±c tiáº¿p vá»›i má»™t dÃ²ng lá»‡nh:
```bash
python -m scripts.inference --unet_config_path "configs/unet/stage2_512.yaml" --inference_ckpt_path "checkpoints/latentsync_unet.pt" --inference_steps 20 --guidance_scale 1.5 --enable_deepcache --video_path "assets/thl2_trimmed.mp4" --audio_path "assets/thl_trimmed.wav" --video_out_path "video_out.mp4"
```

### âš™ï¸ TÃ¹y chá»‰nh tham sá»‘ inference

Äiá»u chá»‰nh cÃ¡c tham sá»‘ sau Ä‘á»ƒ Ä‘áº¡t káº¿t quáº£ tá»‘t nháº¥t:

- **`inference_steps`** [20-50]: GiÃ¡ trá»‹ cao hÆ¡n cáº£i thiá»‡n cháº¥t lÆ°á»£ng hÃ¬nh áº£nh nhÆ°ng cháº­m hÆ¡n
- **`guidance_scale`** [1.0-3.0]: GiÃ¡ trá»‹ cao hÆ¡n cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c lip-sync nhÆ°ng cÃ³ thá»ƒ gÃ¢y mÃ©o hoáº·c giáº­t hÃ¬nh
- **`enable_deepcache`**: Báº­t Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ inference

## ğŸ”„ Data Processing Pipeline

Pipeline xá»­ lÃ½ dá»¯ liá»‡u hoÃ n chá»‰nh bao gá»“m cÃ¡c bÆ°á»›c sau:

1. Loáº¡i bá» cÃ¡c file video bá»‹ lá»—i
2. Resample video FPS vá» 25, vÃ  resample audio vá» 16000 Hz
3. PhÃ¡t hiá»‡n cáº£nh qua [PySceneDetect](https://github.com/Breakthrough/PySceneDetect)
4. Chia má»—i video thÃ nh cÃ¡c Ä‘oáº¡n 5-10 giÃ¢y
5. Affine transform khuÃ´n máº·t dá»±a trÃªn landmarks tá»« [InsightFace](https://github.com/deepinsight/insightface), resize vá» 256Ã—256
6. Loáº¡i bá» video cÃ³ [sync confidence score](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf) tháº¥p hÆ¡n 3, Ä‘iá»u chá»‰nh audio-visual offset vá» 0
7. TÃ­nh [hyperIQA](https://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf) score, loáº¡i bá» video cÃ³ Ä‘iá»ƒm tháº¥p hÆ¡n 40

Cháº¡y script Ä‘á»ƒ thá»±c thi pipeline:

```bash
./data_processing_pipeline.sh
```

**LÆ°u Ã½:** Thay Ä‘á»•i tham sá»‘ `input_dir` trong script Ä‘á»ƒ chá»‰ Ä‘á»‹nh thÆ° má»¥c dá»¯ liá»‡u cáº§n xá»­ lÃ½.

## ğŸ‹ï¸â€â™‚ï¸ Training

### Training U-Net

#### Chuáº©n bá»‹

TrÆ°á»›c khi training, báº¡n cáº§n xá»­ lÃ½ dá»¯ liá»‡u nhÆ° mÃ´ táº£ á»Ÿ pháº§n trÃªn. Táº£i pretrained SyncNet checkpoint:

```bash
huggingface-cli download ByteDance/LatentSync-1.6 stable_syncnet.pt --local-dir checkpoints
```

#### Báº¯t Ä‘áº§u training

```bash
./train_unet.sh
```

#### CÃ¡c config file cÃ³ sáºµn

ThÆ° má»¥c `configs/unet` chá»©a nhiá»u file cáº¥u hÃ¬nh:

- **`stage1.yaml`**: Stage1 training, yÃªu cáº§u **23 GB** VRAM
- **`stage2.yaml`**: Stage2 training hiá»‡u suáº¥t tá»‘i Æ°u, yÃªu cáº§u **30 GB** VRAM
- **`stage2_efficient.yaml`**: Stage2 hiá»‡u quáº£, yÃªu cáº§u **20 GB** VRAM (phÃ¹ há»£p RTX 3090)
- **`stage1_512.yaml`**: Stage1 vá»›i Ä‘á»™ phÃ¢n giáº£i 512Ã—512, yÃªu cáº§u **30 GB** VRAM
- **`stage2_512.yaml`**: Stage2 vá»›i Ä‘á»™ phÃ¢n giáº£i 512Ã—512, yÃªu cáº§u **55 GB** VRAM

#### Táº¡o danh sÃ¡ch file dá»¯ liá»‡u

```bash
python -m tools.write_fileslist
```

### Training SyncNet

Náº¿u muá»‘n train SyncNet trÃªn dataset riÃªng:

```bash
./train_syncnet.sh
```

## ğŸ“Š Evaluation

### ÄÃ¡nh giÃ¡ sync confidence score

```bash
./eval/eval_sync_conf.sh
```

### ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c SyncNet

```bash
./eval/eval_syncnet_acc.sh
```

**LÆ°u Ã½:** Dá»¯ liá»‡u test cáº§n Ä‘Æ°á»£c xá»­ lÃ½ qua pipeline trÆ°á»›c khi Ä‘Ã¡nh giÃ¡.

## ğŸ”¥ Updates

- `2025/06/11`: PhÃ¡t hÃ nh **LatentSync 1.6** - train trÃªn video 512Ã—512 Ä‘á»ƒ giáº£m Ä‘á»™ má». Xem demo [táº¡i Ä‘Ã¢y](docs/changelog_v1.6.md)
- `2025/03/14`: PhÃ¡t hÃ nh **LatentSync 1.5** - cáº£i thiá»‡n tÃ­nh nháº¥t quÃ¡n thá»i gian, hiá»‡u suáº¥t trÃªn video tiáº¿ng Trung, vÃ  giáº£m VRAM xuá»‘ng **20 GB**. Chi tiáº¿t [táº¡i Ä‘Ã¢y](docs/changelog_v1.5.md)

## ğŸ“‘ Open-source Plan

- [x] Inference code vÃ  checkpoints
- [x] Data processing pipeline
- [x] Training code

## ğŸ™ Acknowledgement

- Our code is built on [AnimateDiff](https://github.com/guoyww/AnimateDiff). 
- Some code are borrowed from [MuseTalk](https://github.com/TMElyralab/MuseTalk), [StyleSync](https://github.com/guanjz20/StyleSync), [SyncNet](https://github.com/joonson/syncnet_python), [Wav2Lip](https://github.com/Rudrabha/Wav2Lip).

Thanks for their generous contributions to the open-source community!

## ğŸ“– Citation

If you find our repo useful for your research, please consider citing our paper:

```bibtex
@article{li2024latentsync,
  title={LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision},
  author={Li, Chunyu and Zhang, Chao and Xu, Weikai and Lin, Jingyu and Xie, Jinghui and Feng, Weiguo and Peng, Bingyue and Chen, Cunjian and Xing, Weiwei},
  journal={arXiv preprint arXiv:2412.09262},
  year={2024}
}
```
